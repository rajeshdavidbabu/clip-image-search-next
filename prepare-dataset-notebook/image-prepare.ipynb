{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Specify the path to your .env file\n",
    "env_path = '../.env'\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Read the photos table\n",
    "photos_csv = pd.read_csv(\"dataset/photos.tsv000\", sep='\\t', header=0)\n",
    "\n",
    "# Extract the IDs and the URLs of the photos\n",
    "photos_info_list = photos_csv[['photo_id', 'photo_image_url', 'photo_description', 'photo_width', 'photo_height', 'photo_aspect_ratio', 'photo_location_name', 'ai_description']].values.tolist()\n",
    "\n",
    "# Print some statistics\n",
    "print(f'Photos in the dataset: {len(photos_info_list)}')\n",
    "print(f'Sample photo list-item: {photos_info_list[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the photos in the local folder\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Path where the photos will be downloaded\n",
    "photos_download_path = Path(\"photos\")\n",
    "\n",
    "# Ensure the download directory exists\n",
    "photos_download_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Get list of already downloaded images\n",
    "existing_images = set(file.stem for file in photos_download_path.glob('*.jpg'))\n",
    "\n",
    "# Filter the dataframe to include only images that haven't been downloaded\n",
    "images_to_download = photos_csv[~photos_csv['photo_id'].isin(existing_images)]\n",
    "\n",
    "print(f\"Total images in TSV: {len(photos_csv)}\")\n",
    "print(f\"Already downloaded: {len(existing_images)}\")\n",
    "print(f\"Images to download: {len(images_to_download)}\")\n",
    "\n",
    "def download_photo(photo):\n",
    "    photo_id, photo_url = photo\n",
    "    # Add width specification to the URL\n",
    "    photo_url = photo_url + \"?w=640\"\n",
    "    photo_path = photos_download_path / f\"{photo_id}.jpg\"\n",
    "\n",
    "    if not photo_path.exists():\n",
    "        try:\n",
    "            urllib.request.urlretrieve(photo_url, photo_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot download {photo_url}: {str(e)}\")\n",
    "\n",
    "# Create the thread pool\n",
    "threads_count = 16\n",
    "pool = ThreadPool(threads_count)\n",
    "\n",
    "# Prepare the list of photos to download\n",
    "photos_to_download = list(images_to_download[['photo_id', 'photo_image_url']].itertuples(index=False, name=None))\n",
    "\n",
    "# Start the download with progress bar\n",
    "for _ in tqdm(pool.imap_unordered(download_photo, photos_to_download), total=len(photos_to_download)):\n",
    "    pass\n",
    "\n",
    "# Close the pool\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Display statistics\n",
    "print(f'Photos downloaded: {len(list(photos_download_path.glob(\"*.jpg\")))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "\n",
    "# Get the API key from the environment variable\n",
    "api_key = os.getenv('PINECONE_API_KEY')\n",
    "index_name = os.getenv('PINECONE_INDEX_NAME')\n",
    "\n",
    "# Check if the API key is set\n",
    "if not api_key:\n",
    "    raise ValueError(\"PINECONE_API_KEY environment variable is not set\")\n",
    "\n",
    "# Initialize Pinecone with the API key from the environment variable\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "# List all indexes\n",
    "existing_indexes = pc.list_indexes()\n",
    "\n",
    "print(f\"Existing indexes: {existing_indexes}\")\n",
    "\n",
    "# Check if the index exists\n",
    "if index_name in [index.name for index in existing_indexes]:\n",
    "    print(f\"Index {index_name} already exists\")\n",
    "    index = pc.Index(index_name)\n",
    "else:\n",
    "    # Create the index if it doesn't exist\n",
    "    pc.create_index(\n",
    "      name=index_name,\n",
    "      dimension=768,\n",
    "      metric=\"cosine\",\n",
    "      spec=ServerlessSpec(\n",
    "          cloud='aws',\n",
    "          region='us-east-1'\n",
    "      )\n",
    "    ) \n",
    "    print(f\"Created new index: {index_name}\")\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "print(f\"Index {index_name} ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# Load CLIP model\n",
    "model_id = os.getenv('CLIP_MODEL_ID')\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model = CLIPModel.from_pretrained(model_id)\n",
    "\n",
    "# Move model to device if possible\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Function to create image embeddings\n",
    "def create_image_embeddings(image):\n",
    "    vals = processor(text=None, images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        image_embedding = model.get_image_features(vals)\n",
    "    return image_embedding[0].cpu().numpy().tolist()\n",
    "\n",
    "# Path to your photos folder\n",
    "photos_path = Path(\"photos\")\n",
    "\n",
    "# Create a dictionary for quick lookup\n",
    "photo_info = {\n",
    "    item[0]: {\n",
    "        \"url\": item[1],\n",
    "        \"description\": item[2] if not pd.isna(item[2]) else \"No description available\",\n",
    "        \"width\": item[3],\n",
    "        \"height\": item[4],\n",
    "        \"aspect_ratio\": item[5],\n",
    "        \"location_name\": item[6] if not pd.isna(item[6]) else \"No location available\",\n",
    "        \"ai_description\": item[7] if not pd.isna(item[7]) else \"No AI description available\",\n",
    "    } \n",
    "    for item in photos_info_list\n",
    "}\n",
    "\n",
    "def process_image(img_path):\n",
    "    try:\n",
    "        image = Image.open(img_path)\n",
    "        embedding = create_image_embeddings(image)\n",
    "        photo_id = img_path.stem\n",
    "        info = photo_info.get(photo_id, {})\n",
    "        return {\n",
    "            \"id\": photo_id,\n",
    "            \"values\": embedding,\n",
    "            \"metadata\": {\n",
    "                \"photo_image_url\": info.get(\"url\", \"\"),\n",
    "                \"photo_description\": info.get(\"description\", \"\"),\n",
    "                \"photo_width\": info.get(\"width\", \"\"),\n",
    "                \"photo_height\": info.get(\"height\", \"\"),\n",
    "                \"photo_aspect_ratio\": info.get(\"aspect_ratio\", \"\"),\n",
    "                \"photo_location_name\": info.get(\"location_name\", \"\"),\n",
    "                \"ai_description\": info.get(\"ai_description\", \"\")\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Create the thread pool\n",
    "threads_count = 32\n",
    "pool = ThreadPool(threads_count)\n",
    "\n",
    "# Prepare the list of photos to process\n",
    "photos_to_process = list(photos_path.glob(\"*.jpg\"))\n",
    "\n",
    "# Batch size for upserting to Pinecone\n",
    "batch_size = 500\n",
    "vectors = []\n",
    "total_processed = 0\n",
    "\n",
    "# Start the processing with progress bar\n",
    "for result in tqdm(pool.imap_unordered(process_image, photos_to_process), total=len(photos_to_process)):\n",
    "    if result:\n",
    "        vectors.append(result)\n",
    "        \n",
    "        if len(vectors) >= batch_size:\n",
    "            index.upsert(vectors=vectors, namespace=\"unsplashlite\")\n",
    "            total_processed += len(vectors)\n",
    "            print(f\"\\nUpserted batch of {len(vectors)} vectors. Total processed: {total_processed}\")\n",
    "            print(\"Current index stats:\")\n",
    "            print(index.describe_index_stats())\n",
    "            vectors = []\n",
    "\n",
    "# Close the pool\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Upsert any remaining vectors\n",
    "if vectors:\n",
    "    index.upsert(vectors=vectors, namespace=\"unsplashlite\")\n",
    "    total_processed += len(vectors)\n",
    "    print(f\"\\nUpserted final batch of {len(vectors)} vectors. Total processed: {total_processed}\")\n",
    "    print(\"Final index stats:\")\n",
    "    print(index.describe_index_stats())\n",
    "\n",
    "print(\"\\nProcessing complete!\")\n",
    "print(f'Total photos processed: {total_processed}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
